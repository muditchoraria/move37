{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "class QLearner:\n",
    "    '''Implementation of Q-Learning and Dyna-Q'''\n",
    "    \n",
    "    def __init__(self, *, num_states, num_actions,\n",
    "                 discount_rate = 1.0, random_action_prob = 0.5,\n",
    "                 random_action_decay_rate = 0.99, \n",
    "                 dyna_iterations = 0):\n",
    "        \n",
    "        self._num_states = num_states\n",
    "        self._num_actions = num_actions\n",
    "        self._learning_rate = learning_rate\n",
    "        self._discount_rate = discount_rate\n",
    "        self._random_action_prob = random_action_prob\n",
    "        ####\n",
    "        self._random_action_decay_rate = random_action_decay_rate\n",
    "        ####\n",
    "        self._dyna_iterations = dyna_iterations\n",
    "        \n",
    "        self._experiences = []\n",
    "        \n",
    "        #Initialize Q to small random values\n",
    "        self._Q = np.zeros((num_states, num_actions), dtype = np.float)\n",
    "        self._Q += np.random.normal(0, 0.3, self._Q.shape)\n",
    "        \n",
    "    def initialize(self, state):\n",
    "        '''Set the initial state and return learner's first action'''\n",
    "        self._decide_next_action(state)\n",
    "        self._stored_state = state\n",
    "        return self_stored_action\n",
    "    \n",
    "    def _decide_next_action(self, state):\n",
    "        if rand.random() <= self._random_action_prob:\n",
    "            self._stored_action = random.randint(0, self._num_actions - 1)\n",
    "        else:\n",
    "            self._stored_action = self._find_best_action(state)\n",
    "            \n",
    "    def _find_best_action(self, state):\n",
    "        return int(np.argmax(self._Q[state, :]))\n",
    "    \n",
    "    def learn(self, initial_state, experience_func, iterations = 100):\n",
    "        '''Iteratively experience new states and rewards'''\n",
    "        all_policies = np.zeros((self._num_states, iterations))\n",
    "        all_utilites = np.zeros_like(all_policies)\n",
    "        for i in range(iterations):\n",
    "            done = False\n",
    "            self.initialize(initial_state)\n",
    "            for j in range(iterations):\n",
    "                state, reward, done = experience_func(self._stored_state, self._stored_action)\n",
    "                self.experience(state, reward)\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            policy, utlity = self.get_policy_and_utility()\n",
    "            all_policies[:, i] = policy\n",
    "            all_utilities[:, i] = utility\n",
    "        return all_policies, all_utilites\n",
    "    \n",
    "    def get_policy_and_utility(self):\n",
    "        policy = np.argmax(self._Q, axis=1)\n",
    "        utility = np.max(self._Q, axis=1)\n",
    "        return policy, utility\n",
    "    \n",
    "    def experience(self, state, reward):\n",
    "        '''The learner experiences state and receives a reward'''\n",
    "        self._update_Q(self._stored_state, self._stored_action, state, reward)\n",
    "        \n",
    "        if(self._dyna_iterations > 0):\n",
    "            self._experiences.append(self._stored_state, self._stored_action, state, reward)\n",
    "            exp_idx = np.random.choice(len(self._experinces), self._dyna_iterations)\n",
    "            for i in exp_idx:\n",
    "                self._update_Q(*self._experiences[i])\n",
    "                \n",
    "        #determine an action and update the current state\n",
    "        self._decide_next_action(state)\n",
    "        self._stored_state = state\n",
    "        \n",
    "        self._random_action_prob *= self_random_action_decay_rate\n",
    "        \n",
    "        return self._stored_action\n",
    "    \n",
    "    def _update_Q(self, s, a, s_prime, r):\n",
    "        best_reward = self._Q[s_prime, self._find_best_action(s_prime)]\n",
    "        self._Q[s, a] *= (1 - self._learning_rate)\n",
    "        self._Q[s, a] += (self._learning_rate * (r + self_discount_rate * best_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "class GridWorldMDP:\n",
    "\n",
    "    # up, right, down, left\n",
    "    _direction_deltas = [(-1, 0), (0, 1), (1, 0), (0, -1),]\n",
    "    _num_actions = len(_direction_deltas)\n",
    "    \n",
    "    def __init__(self, reward_grid, terminal_mask, obstacle_mask, action_probabilites, no_action_probability):\n",
    "        self._reward_grid = reward_grid\n",
    "        self._terminal_mask = terminal_mask\n",
    "        self._obstacle_mask = obstacle_mask\n",
    "        self._T = self._create_transition_matrix(action_probabilities, no_action_probability obstacle_mask)\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._reward_grid.shape\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._reward_grid.size\n",
    "\n",
    "    @property\n",
    "    def reward_grid(self):\n",
    "        return self._reward_grid\n",
    "    \n",
    "    def _create_transition_matrix(self, action_probabilities, no_action_probability obstacle_mask):\n",
    "        M, N = self.shape\n",
    "        \n",
    "        T = np.zeros((M, N, self._num_actions, M, N))\n",
    "        r0, c0 = self.grid_indices_to_coordinates()\n",
    "        T[r0, c0, :, r0, c0] += no_action_probability\n",
    "        \n",
    "        for action in range(self._num_actions):\n",
    "            for offset, P in action_probabilities:\n",
    "                direction = (action + offset) % self._num_actions\n",
    "                \n",
    "                dr, dc = self._direction_deltas[direction]\n",
    "                r1 = np.clip(r0 + dr, 0, M - 1)\n",
    "                c1 = np.clip(c0 + dc, 0, N - 1)\n",
    "                \n",
    "                temp_mask = obstacle_mask[r1, c1].flatten()\n",
    "                r1[temp_mask] = r0[temp_mask]\n",
    "                c1[temp_mask] = c0[temp_mask]\n",
    "\n",
    "                T[r0, c0, action, r1, c1] += P\n",
    "\n",
    "        terminal_locs = np.where(self._terminal_mask.flatten())[0]\n",
    "        T[r0[terminal_locs], c0[terminal_locs], :, :, :] = 0\n",
    "        return T\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
