{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Reinforcement learning lies *in between* supervised and unsupervised learning. While the former has a label and the latter does not, reinforcement learning has *sparse time delay labels* (rewards). Reinforcement learning is about creating a mathematical framework that encapsulates the idea of an AI interacting with an environment and *time* acting as a dimension and learning through **trial and error**.\n",
    "\n",
    "Markov chain is a set of states moved successively from one state to another using what we call a **transition matrix** one step at a time. Future is conditionally independent on any **previous states** given the current state. The current state encapsulates all that is needed to decide the future state when an input action is received. eg - a chess configuration.\n",
    "\n",
    "The representation of RL is **Markov decision process**. (Extenstion by addition of decision and reward elements)\n",
    "Five components - set of states, transitions, set of actions, starting state, reward.\n",
    "\n",
    "*State* - Set of tokens that represent every possible condition.\n",
    "\n",
    "*Action* - Set of all possible decisions. (a->whole, a(S) -> in a particular state) (Stochastic (SxA->Prob(S)) and Deterministic (SxA->S') )\n",
    "\n",
    "*Model* - Action's effect. In particular, T(S, a, S’) defines a transition T where being in state S and taking an action ‘a’ takes us to state S’ (S and S’ may be same). For stochastic actions (noisy, non-deterministic) we also define a probability P(S’|S,a) (transition probablity matrix) which represents the probability of reaching a state S’ if action ‘a’ is taken in state S. \n",
    "*Transition matrix is like a next-state function but every state is thought to be a possible consequence of an action in a state*\n",
    "\n",
    "*Reward* - Real-valued response, denoted by R(S, a, S').\n",
    "\n",
    "*Discount factor* ($\\gamma$) - It represents the relative importance of the current rewards wrt the future rewards. It fugres out the choice between performing an action that yields immediate rewards but leads to a less rewarding state or go with the other one. This helps us in achieving the **infinite-horizon** optimal solution. (A k-horizon solution ==> Policy that results in maximal expected reward sum from time 0 to k). So discount implies ==> worth of reward decreases with time. So it guarantees the convergence of the algorithm.\n",
    "\n",
    "*Policy* - It is the solution to our MDP. It is a set of actions that are taken to reach a goal. It indicates the action to be taken in a particular state. A policy is denoted as ‘Pi’ π(s) –> ∞. \n",
    "π\\* is called the optimal policy, one that optimizes to maximise the reward expected to be received at the end time.\n",
    "**The policy isa guide that tells you what action to take at a point. It is not a plan but uncovers the plan by returning actions to take for each state. So no matter where you are it suggests the action that's best there.**\n",
    "\n",
    "Put the states in a grid ==> Capture the essence of the environment by dividing it into the five components ==> Take decisions ==> The solution is policy.\n",
    "\n",
    "Goal - Choose an optimal action that maximises the long term expected reward\n",
    "\n",
    "Summary - an AI learns how to optimally interact in a real-time environment using time delayed labels called rewards as a signal\n",
    "The Markov decision process is a mathematical framework for defining the RL problem using states, action and reward\n",
    "Through interaction, an AI will learn a policy that will return the action for a given state with the highest reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation\n",
    "\n",
    "STATE: a numeric representation of what the agent is observing al a particular point of time in the environment, eg, current pixel values\n",
    "ACTION: the input the agent provides to the environment, calculated by applying a policy to the current state, eg, a joystick press\n",
    "REWARD: a feedback signal from the environment reflecting how well the agent is performing the goals of the game, eg, score or enemies killed\n",
    "\n",
    "RL is based on Dynamic programming introduced by Dr. Richard Bellman. DP helps to solve complex problem by simpliying it into smaller subproblems recursively.\n",
    "\n",
    "What does this equation solve ? \n",
    "Ans - It evaluates the *value* of a state. It helps us to find the *expected reward* of a state *relative* to the advantage or disadvantage of each state. *Note* that even on taking the best possible / optimum action at a point may not give the expected reward\n",
    "\n",
    "**Explaination of basic bellman equation follows here\n",
    "\n",
    "Goal -  Finding the optimal action which will maximize the value of the expected long-term discounted rewards\n",
    "\n",
    "Use NN to estimate the value of a state ==> Guess which one maximises the expected reward\n",
    "\n",
    "**Tips for gamma.**\n",
    "\n",
    "Finetune gamma between 0.9 and 0.99\n",
    "A lower value encourages short-term thinking while a higher value emphasizes future rewards.\n",
    "'Sense of urgency in the real world'- that is what discount factor is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function\n",
    "\n",
    "The value function or value of policy tells us \"how good\" a state is for an agent. It is equal to the **expected discounted reward** when starting from state 's' and following a policy 'π' for an action. This completely depends on the policy chosen.\n",
    "\n",
    "Types :-\n",
    "-> State-Value function: *How good is a state*\n",
    "-> Action-Value function: *How good is a state-action pair* (Q-function)\n",
    "\n",
    "Optimal Value function: Value of state that maximises the discounted reward. (V\\*(s))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
