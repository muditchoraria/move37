{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Value iteration is a method of computing the optimal MDP policy and it's value\n",
    "It is basically Dynamic Programming\n",
    "\n",
    "We start at the end and then work our way backwards, updating the value of Q\\* or V\\*. That is an arbitrary end point. Assuming we have k stages to go, and let Qk be the Q-functions for k stages. We define it recursively and get the equations for the next k stages.\n",
    "\n",
    "Qk+1(s,a)\t= ∑s' P(s'|s,a) (R(s,a,s')+ γVk(s'))  for k ≥ 0\n",
    "Vk(s)\t    = max a Qk(s,a)  for k>0.\n",
    "\n",
    "1:           assign V0[S] arbitrarily \n",
    "2:           k ←0 \n",
    "3:           repeat\n",
    "4:                     k ←k+1 \n",
    "5:                     for each state s do \n",
    "6:                               Vk[s] = maxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk-1[s']) \n",
    "7:           until ∀s |Vk[s]-Vk-1[s]| < θ \n",
    "8:           for each state s do \n",
    "9:                     π[s] = argmaxa ∑s' P(s'|s,a) (R(s,a,s')+ γVk[s']) \n",
    "10:          return π,Vk\n",
    "\n",
    "\n",
    "What value iteration does is it sets the reward for all the states (typically, X for goal state, 0 for others). Then the utility value of a state gets distributed step by step backwards. Namely, they will get a Utility equal to the probability that from that state we can get to the goal stated. We then continue iterating, at each step we move the utility back 1 more step away from the goal.\n",
    "\n",
    "Value iteration computes the optimal state value function by iteratively improving the estimate of V(s). The algorithm initialize V(s) to arbitrary random values. It repeatedly updates the Q(s, a) and V(s) values until they convergs. Value iteration is guranteed to converge to the optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s simplify things a little bit first.\n",
    "\n",
    "First of all, for now, let’s ignore this pesky discount factor λ.\n",
    "\n",
    "Second, a MDP is used to reason about the stochastic world, where we can take some actions and we are not exactly sure where we will end up. But what if we lived in a deterministic world, in which every action from some state s is known to get us to a state s′? How would things look in this simplified world?\n",
    "\n",
    "Well, in this deterministic world, we know that taking that an action a from state s will for sure take me to some other state s′, and we will obtain a reward of R(s,a,s′). And now we’ve landed on a new state s′ with a value of V(s′). Remember that we also have multiple actions we can choose from, but our optimal value is achieved by taking the action that leads to the maximum overall utility. In such a world, it’s not too difficult to see that our update equation for the optimal value of state should be the following:\n",
    "\n",
    "Vk+1(s)=maxaR(s,a,s′)+Vk(s′)\n",
    "\n",
    "In more simple terms, the optimal value of our current state is just the value of the new state we land in added to the reward we obtain from going to that state, and we take the best action from all possible actions. The notation maxa is a formal way of expressing this idea of picking the best value from a set of actions.\n",
    "\n",
    "Now, let’s add in this notion of a decay factor λ.\n",
    "\n",
    "The decay factor has a fairly intuitive meaning in that in most real world MDP’s, we make an implicit assumption that it’s better to get a reward sooner rather than later. A very natural way to model this is to have the value of rewards decay exponentially. This decay factor also ensures that the values of states converge — otherwise, we could obtain infinite utilities by repeatedly gaining rewards for actions.\n",
    "\n",
    "All of this is achieved by multiplying the value obtained from subsequent states by λ. With the discount factor, the update equation is modified to be\n",
    "\n",
    "Vk+1(s)=maxaR(s,a,s′)+λVk(s′)\n",
    "\n",
    "Notice that as we continue through time steps, our values of states we see in future time steps decay exponentially.\n",
    "\n",
    "The last piece of the puzzle in the Bellman equation is factoring in a stochastic world. In such a world, we could take an action a from state s, but end up in various possible states s′. We encode this uncertainty through a probability distribution.\n",
    "\n",
    "That is, for every possible s′ from state s resulting from an action a, we have a transition probability to that state T(s,a,s′).\n",
    "\n",
    "Now, remember in the deterministic world, we had the following update equation:\n",
    "\n",
    "Vk+1(s)=maxaR(s,a,s′)+λVk(s′)\n",
    "\n",
    "Now in the stochastic world, we have to factor in the rewards and values of many possible states, but each one have a defined probability of getting to that state. As a result, we have to take an expected value. Informally, this is simply the sum of the values of each of these possible states, weighted by their probabilities.\n",
    "\n",
    "Mathematically, our update equation translates into the following:\n",
    "\n",
    "Vk+1(s)=maxa∑s′T(s,a,s′)[R(s,a,s′)+λVk(s′)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_______________________________\n",
    "\n",
    "Reinforcement learning has two problems - first the prediction problem in other words we need to calculate accurate values for each possible state second there's a control problem where we need to find the optimal policy which leads to the highest expected rewards these two go hand in hand and often depend on each other to reach an optimal solution when we have imperfect data about the environment\n",
    "\n",
    "there are two classes of dynamic program algorithms to solve this problem policy iteration and value iteration policy iteration starts out with a totally random policy and starts taking actions it then starts estimating values for each square based on the reward received from these random actions updating the value table and using improved values to calculate and improve policy this continues until both the policy and value tables stabilize and stop changing another way to say this is that our algorithm converges value iteration completely ignores policy and focuses on applying the bellman equation exactly as we learned it to perfectly calculate values for each square we can then calculate a perfect policy in one pass for each state by finding the action with the highest expected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
